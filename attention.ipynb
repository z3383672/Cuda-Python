{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "def replicate(layer,N):\n",
    "    return nn.ModuleList([layer for _ in range(N)])\n",
    "\n",
    "class Encode(nn.Module):\n",
    "\n",
    "    def __init__(self,layer,N):\n",
    "        super(Encode, self).__init__()\n",
    "        self.layer=replicate(layer,N)\n",
    "        self.N=N\n",
    "\n",
    "    def forward(self,X,mask):\n",
    "        for layer in self.layer:\n",
    "            X=layer(X,mask)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self,features,eps=1e-16):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2=nn.Parameter(torch.ones(features))\n",
    "        self.b_2=nn.Parameter(torch.zeros(features))\n",
    "        self.eps=eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, size, dropout_rate, eps=1e-6):\n",
    "        super(AddNorm, self).__init__()\n",
    "        \n",
    "        # Layer normalization component\n",
    "        self.norm = LayerNorm(size, eps=eps)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, sublayer_output):\n",
    "        \"Apply residual connection followed by layer normalization\"\n",
    "        # Residual connection\n",
    "        added_output = x + self.dropout(sublayer_output)\n",
    "        \n",
    "        # Layer normalization\n",
    "        return self.norm(added_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        d_ff: the number of features of the feedforward network model.\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        # Two linear layers with a ReLU activation in between\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def attention(query,Q,K,V):\n",
    "    #K transpose might cause a trpoouble if we have multi dimntioanmatrix\n",
    "    scores=torch.matmul(Q,K.T)/math.sqrt(d_k)\n",
    "    p_atten=scores.softmax(scores, dim=2)\n",
    "    return torch.matmul(p_atten,V),p_atten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key,value,mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Linear layers\n",
    "        Q = self.query(query)\n",
    "        K = self.key(key)\n",
    "        V = self.value(value)\n",
    "\n",
    "        # Split into multiple heads\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attention_scores = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.d_k**0.5\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_output = torch.matmul(attention_probs, V)\n",
    "\n",
    "        # Concatenate heads and pass through final linear layer\n",
    "        attention_output = attention_output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_k * self.num_heads)\n",
    "        output = self.fc_out(attention_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, size, FeedForward, Multi_Head_Attention,AddNorm):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.self_attn = Multi_Head_Attention\n",
    "        self.feed_forward = FeedForward\n",
    "        self.AddNorm=AddNorm\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        x=self.AddNorm(x, self.self_attn(x,x,x,mask))\n",
    "        x=self.AddNorm(x, self.feed_forward(x))\n",
    "        return x      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src=torch.LongTensor([0,1,2,3,4,5,6,7,8,9])\n",
    "# tgt=torch.LongTensor([0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "# model=Encoder_Decoder(\n",
    "#         Encode(Encoder(512,FeedForward(512, 2048, dropout=0.1), MultiHeadAttention(512, 8),AddNorm(512, 0.1, eps=1e-6)),6),\n",
    "#         Decode(Decoder(512, FeedForward(512, 2048, dropout=0.1), MultiHeadAttention(512, 8),MultiHeadAttention(512,8)),6),\n",
    "#         nn.Sequential(Embedding(512,10),Positional_Encoding(512)),\n",
    "#         nn.Sequential(Embedding(512,10),Positional_Encoding(512)),\n",
    "#         Generator(512,10)\n",
    "#     )\n",
    "# A=model.src_embed(src)\n",
    "# B=model.Encode(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 512])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B=MultiHeadAttention(512, 8)\n",
    "# B(A,A,A).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this might be a replicate\n",
    "class Decode(nn.Module):\n",
    "\n",
    "    def __init__(self,layer,N):\n",
    "        super(Decode, self).__init__()\n",
    "        self.layer=replicate(layer,N)\n",
    "        self.N=N\n",
    "\n",
    "    def forward(self,X,y,src_mask,tgt_mask):\n",
    "        for layer in self.layer:\n",
    "            X=layer(X,y,src_mask,tgt_mask)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, size, FeedForward, Self_Multi_Head_Attention,Encoder_Multi_Head_Attention):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.self_attn = Self_Multi_Head_Attention\n",
    "        self.feed_forward = FeedForward\n",
    "        self.encoder_attention=Encoder_Multi_Head_Attention\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self,x,m,src_mask,tgt_mask):\n",
    "        x=self.AddNorm(x, self.self_attn(x,x,x,tgt_mask))\n",
    "        x=self.AddNorm(x, self.encoder_attention(x,m,m,src_mask))\n",
    "        x=self.AddNorm(x, FeedForward(x))\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, d_model,vocab):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Positional_Encoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Positional_Encoding, self).__init__()\n",
    "        self.dmodel=d_model\n",
    "        \n",
    "    def forward(self,x):\n",
    "        n=x.shape[0]\n",
    "        div_term = torch.exp(torch.arange(0., self.dmodel, 2) * -(math.log(10000.0) / self.dmodel))\n",
    "\n",
    "        positions = torch.arange(n).unsqueeze(1).float()\n",
    "        div_term = div_term.unsqueeze(0)\n",
    "        sin_vals = torch.sin(positions * div_term)\n",
    "        cos_vals = torch.cos(positions * div_term)\n",
    "\n",
    "        ZZ = torch.empty(n, self.dmodel)\n",
    "        ZZ[:, 0::2] = sin_vals\n",
    "        ZZ[:, 1::2] = cos_vals\n",
    "        return x+ZZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Positional_Encoding(Embeded):\n",
    "#     n=Embeded.shape[0]\n",
    "#     dmodel=Embeded.shape[1]\n",
    "#     div_term = torch.exp(torch.arange(0., dmodel, 2) * -(math.log(10000.0) / dmodel))\n",
    "\n",
    "#     # Expand dimensions for broadcasting\n",
    "#     positions = torch.arange(n).unsqueeze(1).float()\n",
    "#     div_term = div_term.unsqueeze(0)\n",
    "\n",
    "#     # Calculate sin and cos values\n",
    "#     sin_vals = torch.sin(positions * div_term)\n",
    "#     cos_vals = torch.cos(positions * div_term)\n",
    "\n",
    "#     # Interleave sin and cos values\n",
    "#     ZZ = torch.empty(n, dmodel)\n",
    "#     ZZ[:, 0::2] = sin_vals\n",
    "#     ZZ[:, 1::2] = cos_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Decoder(nn.Module):\n",
    "    def __init__(self, Encode,Decode,src_embed, tgt_embed, generator):\n",
    "        super(Encoder_Decoder, self).__init__()\n",
    "        self.Encode=Encode\n",
    "        self.Decode=Decode\n",
    "        self.src_embed=src_embed\n",
    "        self.tgt_embed=tgt_embed\n",
    "        self.generator=generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.Encode(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.Decode(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,d_model,tgt_vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.linear=nn.Linear(d_model,tgt_vocab)\n",
    "\n",
    "    def forward(sefl,x):\n",
    "        x=self.linear(x)\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab,tgt_vocab,N=6,d_model=512,h=8,dropout=0.1,d_ff=2048):\n",
    "    \n",
    "    model=Encoder_Decoder(\n",
    "        Encode(Encoder(d_model,FeedForward(d_model, d_ff, dropout=dropout), MultiHeadAttention(d_model, h),AddNorm(d_model,dropout, eps=1e-6)),N),\n",
    "        Decode(Decoder(d_model, FeedForward(d_model, d_ff, dropout=dropout), MultiHeadAttention(d_model, h),MultiHeadAttention(d_model,h)),N),\n",
    "        nn.Sequential(Embedding(d_model,src_vocab),Positional_Encoding(d_model)),\n",
    "        nn.Sequential(Embedding(d_model,tgt_vocab),Positional_Encoding(d_model)),\n",
    "        Generator(d_model,tgt_vocab)\n",
    "    )\n",
    "    for p in model.parameters():\n",
    "        if p.dim()>1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-353-b8f474b3ede9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     out = test_model.decode(\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsequent_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         )\n",
      "\u001b[1;32m<ipython-input-341-d19b092c37be>\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, memory, src_mask, tgt, tgt_mask)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtgt_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\61401\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\61401\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\61401\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-300-9c726b922ec2>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\61401\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\61401\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32mc:\\Users\\61401\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2208\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "test_model = make_model(11, 11, 2)\n",
    "test_model.eval()\n",
    "src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "src_mask = torch.ones(1, 1, 10)\n",
    "memory = test_model.encode(src, src_mask)\n",
    "ys = torch.zeros(1, 1).type_as(src)\n",
    "\n",
    "\n",
    "for i in range(9):\n",
    "    out = test_model.decode(\n",
    "        memory, src_mask, embed_ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "    prob = test_model.generator(out[:, -1])\n",
    "    _, next_word = torch.max(prob, dim=1)\n",
    "    next_word = next_word.data[0]\n",
    "    ys = torch.cat(\n",
    "        [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "    )\n",
    "\n",
    "print(\"Example Untrained Model Prediction:\", ys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import math \n",
    "import torch.nn.functional as F\n",
    "\n",
    "def replicate(layer,N):\n",
    "    return nn.ModuleList([layer for _ in range(N)])\n",
    "\n",
    "class Encode(nn.Module):\n",
    "\n",
    "    def __init__(self,layer,N):\n",
    "        super(Encode, self).__init__()\n",
    "        self.layer=replicate(layer,N)\n",
    "        self.N=N\n",
    "\n",
    "    def forward(self,X,mask):\n",
    "        for layer in self.layer:\n",
    "            X=layer(X,mask)\n",
    "        return X\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self,features,eps=1e-16):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2=nn.Parameter(torch.ones(features))\n",
    "        self.b_2=nn.Parameter(torch.zeros(features))\n",
    "        self.eps=eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "    \n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, size, dropout_rate, eps=1e-6):\n",
    "        super(AddNorm, self).__init__()\n",
    "        \n",
    "        # Layer normalization component\n",
    "        self.norm = LayerNorm(size, eps=eps)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, sublayer_output):\n",
    "        \"Apply residual connection followed by layer normalization\"\n",
    "        # Residual connection\n",
    "        added_output = x + self.dropout(sublayer_output)\n",
    "        \n",
    "        # Layer normalization\n",
    "        return self.norm(added_output)\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        d_ff: the number of features of the feedforward network model.\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        # Two linear layers with a ReLU activation in between\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "    \n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key,value,mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Linear layers\n",
    "        Q = self.query(query)\n",
    "        K = self.key(key)\n",
    "        V = self.value(value)\n",
    "\n",
    "        # Split into multiple heads\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attention_scores = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.d_k**0.5\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_output = torch.matmul(attention_probs, V)\n",
    "\n",
    "        # Concatenate heads and pass through final linear layer\n",
    "        attention_output = attention_output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_k * self.num_heads)\n",
    "        output = self.fc_out(attention_output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, size, FeedForward, Multi_Head_Attention,AddNorm):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.self_attn = Multi_Head_Attention\n",
    "        self.feed_forward = FeedForward\n",
    "        self.AddNorm=AddNorm\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        x=self.AddNorm(x, self.self_attn(x,x,x,mask))\n",
    "        x=self.AddNorm(x, self.feed_forward(x))\n",
    "        return x  \n",
    "    \n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, d_model,vocab):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x)   \n",
    "    \n",
    "class Positional_Encoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Positional_Encoding, self).__init__()\n",
    "        self.dmodel=d_model\n",
    "        \n",
    "    def forward(self,x):\n",
    "        n=x.shape[0]\n",
    "        div_term = torch.exp(torch.arange(0., self.dmodel, 2) * -(math.log(10000.0) / self.dmodel))\n",
    "\n",
    "        positions = torch.arange(n).unsqueeze(1).float()\n",
    "        div_term = div_term.unsqueeze(0)\n",
    "        sin_vals = torch.sin(positions * div_term)\n",
    "        cos_vals = torch.cos(positions * div_term)\n",
    "\n",
    "        ZZ = torch.empty(n, self.dmodel)\n",
    "        ZZ[:, 0::2] = sin_vals\n",
    "        ZZ[:, 1::2] = cos_vals\n",
    "        return x+ZZ\n",
    "    \n",
    "def make_model(src_vocab,tgt_vocab,N=6,d_model=512,h=8,dropout=0.1,d_ff=2048):\n",
    "    \n",
    "    model=Encoder_Decoder(\n",
    "        Encode(Encoder(d_model,FeedForward(d_model, d_ff, dropout=dropout), MultiHeadAttention(d_model, h),AddNorm(d_model,dropout, eps=1e-6)),N),\n",
    "        Decode(Decoder(d_model, FeedForward(d_model, d_ff, dropout=dropout), MultiHeadAttention(d_model, h),MultiHeadAttention(d_model,h)),N),\n",
    "        nn.Sequential(Embedding(d_model,src_vocab),Positional_Encoding(d_model)),\n",
    "        nn.Sequential(Embedding(d_model,tgt_vocab),Positional_Encoding(d_model)),\n",
    "        Generator(d_model,tgt_vocab)\n",
    "    )\n",
    "    for p in model.parameters():\n",
    "        if p.dim()>1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,d_model,tgt_vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.linear=nn.Linear(d_model,tgt_vocab)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.linear(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "    \n",
    "\n",
    "class Encoder_Decoder(nn.Module):\n",
    "    def __init__(self, Encode,Decode,src_embed, tgt_embed, generator):\n",
    "        super(Encoder_Decoder, self).__init__()\n",
    "        self.Encode=Encode\n",
    "        self.Decode=Decode\n",
    "        self.src_embed=src_embed\n",
    "        self.tgt_embed=tgt_embed\n",
    "        self.generator=generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.Encode(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.Decode(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "class Decode(nn.Module):\n",
    "\n",
    "    def __init__(self,layer,N):\n",
    "        super(Decode, self).__init__()\n",
    "        self.layer=replicate(layer,N)\n",
    "        self.N=N\n",
    "\n",
    "    def forward(self,X,y,src_mask,tgt_mask):\n",
    "        for layer in self.layer:\n",
    "            X=layer(X,y,src_mask,tgt_mask)\n",
    "        return X\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, size, FeedForward, Self_Multi_Head_Attention,Encoder_Multi_Head_Attention,AddNorm):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.self_attn = Self_Multi_Head_Attention\n",
    "        self.feed_forward = FeedForward\n",
    "        self.encoder_attention=Encoder_Multi_Head_Attention\n",
    "        self.AddNorm=AddNorm\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self,x,m,src_mask,tgt_mask):\n",
    "        x=self.AddNorm(x, self.self_attn(x,x,x,tgt_mask))\n",
    "        x=self.AddNorm(x, self.encoder_attention(x,m,m,src_mask))\n",
    "        x=self.AddNorm(x, self.feed_forward(x))\n",
    "        return x   \n",
    "    \n",
    "def make_model(src_vocab,tgt_vocab,N=6,d_model=512,h=8,dropout=0.1,d_ff=2048):\n",
    "    \n",
    "    model=Encoder_Decoder(\n",
    "        Encode(Encoder(d_model,FeedForward(d_model, d_ff, dropout=dropout), MultiHeadAttention(d_model, h),AddNorm(d_model,dropout, eps=1e-6)),N),\n",
    "        Decode(Decoder(d_model, FeedForward(d_model, d_ff, dropout=dropout), MultiHeadAttention(d_model, h),MultiHeadAttention(d_model,h),AddNorm(d_model,dropout, eps=1e-6)),N),\n",
    "        nn.Sequential(Embedding(d_model,src_vocab),Positional_Encoding(d_model)),\n",
    "        nn.Sequential(Embedding(d_model,tgt_vocab),Positional_Encoding(d_model)),\n",
    "        Generator(d_model,tgt_vocab)\n",
    "    )\n",
    "    for p in model.parameters():\n",
    "        if p.dim()>1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    data=torch.randint(1,11,size=(50,10))\n",
    "    data[:,0]=1\n",
    "    src=data.requires_grad_(False).clone().detach()\n",
    "    tgt=data.requires_grad_(False).clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5584)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the loss function with reduction set to 'sum'\n",
    "loss_function = nn.KLDivLoss(reduction='sum')\n",
    "\n",
    "# Define two probability distributions (as PyTorch tensors)\n",
    "# Input (log probabilities)\n",
    "input = torch.tensor([[0.2, 0.7, 0.1], [0.9, 0.05, 0.05]], dtype=torch.float).log()\n",
    "# Target (probabilities)\n",
    "target = torch.tensor([[0.1, 0.8, 0.1], [0.5, 0.2, 0.3]], dtype=torch.float)\n",
    "\n",
    "# Compute the loss\n",
    "loss = loss_function(input, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=5\n",
    "padding_idx=0\n",
    "smoothing=0.4\n",
    "x=torch.tensor([1,4,5,6,9,10])\n",
    "predict=torch.FloatTensor([[0,0.2,0.7,0.1,0],\n",
    "                           [0,0.2,0.7,0.1,0],\n",
    "                           [0,0.2,0.7,0.1,0],\n",
    "                           [0,0.2,0.7,0.1,0],\n",
    "                           [0,0.2,0.7,0.1,0],\n",
    "                           [0,0.2,0.7,0.1,0]])\n",
    "\n",
    "true_dist = x.data.clone()\n",
    "true_dist.fill_(smoothing / (size - 2))\n",
    "true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "true_dist[:, self.padding_idx] = 0\n",
    "mask = torch.nonzero(target.data == self.padding_idx)\n",
    "if mask.dim() > 0:\n",
    "    true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "true_dist = true_dist\n",
    "return loss_function(x, true_dist.clone().detach())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
